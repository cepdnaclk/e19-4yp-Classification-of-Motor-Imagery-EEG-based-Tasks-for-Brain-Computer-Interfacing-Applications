{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8febc6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "ðŸ“Š Train Accuracy: 0.9357\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   pronation       0.92      0.96      0.94      2800\n",
      "  supination       0.95      0.92      0.93      2800\n",
      "\n",
      "    accuracy                           0.94      5600\n",
      "   macro avg       0.94      0.94      0.94      5600\n",
      "weighted avg       0.94      0.94      0.94      5600\n",
      "\n",
      "\n",
      "ðŸ“Š Test Accuracy: 0.5050\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   pronation       0.50      0.54      0.52       700\n",
      "  supination       0.51      0.47      0.49       700\n",
      "\n",
      "    accuracy                           0.51      1400\n",
      "   macro avg       0.51      0.51      0.50      1400\n",
      "weighted avg       0.51      0.51      0.50      1400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MLP complex neural network on processed data 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# === CONFIG ===\n",
    "data_dir = '/storage/projects1/e19-4yp-mi-eeg-for-bci/ashan/processed_data2'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# === Load File List and Labels ===\n",
    "npz_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.npz')])\n",
    "\n",
    "all_labels = []\n",
    "for file in npz_files:\n",
    "    y = np.load(file)['y']\n",
    "    all_labels.extend(y)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "classes = np.unique(all_labels)\n",
    "class_weights_array = compute_class_weight(class_weight='balanced', classes=classes, y=all_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float32).to(device)\n",
    "\n",
    "# === Train-Test Split ===\n",
    "train_files, test_files = train_test_split(npz_files, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# === Fit Scaler on Training Data ===\n",
    "scaler = StandardScaler()\n",
    "sample_data = []\n",
    "for file in train_files[:3]:\n",
    "    X = np.load(file)['X']\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    sample_data.append(X)\n",
    "scaler.fit(np.vstack(sample_data))\n",
    "\n",
    "# === Define a More Complex Model ===\n",
    "class ComplexEEGNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ComplexEEGNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Train the Model ===\n",
    "model = ComplexEEGNet(input_dim=sample_data[0].shape[1], num_classes=len(classes)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# === Training Loop ===\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for file in train_files:\n",
    "        data = np.load(file)\n",
    "        X = data['X'].reshape(data['X'].shape[0], -1)\n",
    "        y = data['y']\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "        for start in range(0, len(X_tensor), batch_size):\n",
    "            end = start + batch_size\n",
    "            X_batch = X_tensor[start:end]\n",
    "            y_batch = y_tensor[start:end]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def evaluate(files, label=\"Test\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for file in files:\n",
    "            data = np.load(file)\n",
    "            X = data['X'].reshape(data['X'].shape[0], -1)\n",
    "            y = data['y']\n",
    "            X = scaler.transform(X)\n",
    "\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "            outputs = model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            y_true.extend(y)\n",
    "            y_pred.extend(predictions)\n",
    "\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"\\nðŸ“Š {label} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['pronation', 'supination']))\n",
    "    return acc\n",
    "\n",
    "# === Final Evaluation ===\n",
    "train_acc = evaluate(train_files, \"Train\")\n",
    "test_acc = evaluate(test_files, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "723929b9-4cd1-4277-8241-681990d6569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 - Loss: 1.3973\n",
      "Epoch 2/20 - Loss: 1.3828\n",
      "Epoch 3/20 - Loss: 1.3811\n",
      "Epoch 4/20 - Loss: 1.3697\n",
      "Epoch 5/20 - Loss: 1.3550\n",
      "Epoch 6/20 - Loss: 1.3390\n",
      "Epoch 7/20 - Loss: 1.3107\n",
      "Epoch 8/20 - Loss: 1.2985\n",
      "Epoch 9/20 - Loss: 1.2741\n",
      "Epoch 10/20 - Loss: 1.2417\n",
      "Epoch 11/20 - Loss: 1.2335\n",
      "Epoch 12/20 - Loss: 1.2083\n",
      "Epoch 13/20 - Loss: 1.1877\n",
      "Epoch 14/20 - Loss: 1.1661\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'_thread.RLock' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m train_files:\n\u001b[1;32m     76\u001b[0m     data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(file)\n\u001b[0;32m---> 77\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     78\u001b[0m     y \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     79\u001b[0m     X \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m/storage/projects1/e19-4yp-mi-eeg-for-bci/env/eeg-env/lib/python3.10/site-packages/numpy/lib/npyio.py:256\u001b[0m, in \u001b[0;36mNpzFile.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mMAGIC_PREFIX:\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28mbytes\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mopen(key)\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mbytes\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzip\u001b[38;5;241m.\u001b[39mread(key)\n",
      "File \u001b[0;32m/storage/projects1/e19-4yp-mi-eeg-for-bci/env/eeg-env/lib/python3.10/site-packages/numpy/lib/format.py:831\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    829\u001b[0m             read_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_read_count, count \u001b[38;5;241m-\u001b[39m i)\n\u001b[1;32m    830\u001b[0m             read_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(read_count \u001b[38;5;241m*\u001b[39m dtype\u001b[38;5;241m.\u001b[39mitemsize)\n\u001b[0;32m--> 831\u001b[0m             data \u001b[38;5;241m=\u001b[39m \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43marray data\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m             array[i:i\u001b[38;5;241m+\u001b[39mread_count] \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mfrombuffer(data, dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    833\u001b[0m                                                      count\u001b[38;5;241m=\u001b[39mread_count)\n\u001b[1;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fortran_order:\n",
      "File \u001b[0;32m/storage/projects1/e19-4yp-mi-eeg-for-bci/env/eeg-env/lib/python3.10/site-packages/numpy/lib/format.py:966\u001b[0m, in \u001b[0;36m_read_bytes\u001b[0;34m(fp, size, error_template)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[1;32m    963\u001b[0m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 966\u001b[0m         r \u001b[38;5;241m=\u001b[39m \u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    967\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m r\n\u001b[1;32m    968\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m==\u001b[39m size:\n",
      "File \u001b[0;32m/storage/projects1/e19-4yp-mi-eeg-for-bci/env/eeg-env/lib/python3.10/zipfile.py:930\u001b[0m, in \u001b[0;36mZipExtFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eof:\n\u001b[0;32m--> 930\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_readbuffer \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/storage/projects1/e19-4yp-mi-eeg-for-bci/env/eeg-env/lib/python3.10/zipfile.py:998\u001b[0m, in \u001b[0;36mZipExtFile._read1\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    996\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlen\u001b[39m(data):\n\u001b[0;32m--> 998\u001b[0m         data \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1000\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read2(n)\n",
      "File \u001b[0;32m/storage/projects1/e19-4yp-mi-eeg-for-bci/env/eeg-env/lib/python3.10/zipfile.py:1030\u001b[0m, in \u001b[0;36mZipExtFile._read2\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m   1027\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mMIN_READ_SIZE)\n\u001b[1;32m   1028\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(n, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left)\n\u001b[0;32m-> 1030\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fileobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compress_left \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data)\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n",
      "File \u001b[0;32m/storage/projects1/e19-4yp-mi-eeg-for-bci/env/eeg-env/lib/python3.10/zipfile.py:749\u001b[0m, in \u001b[0;36m_SharedFile.read\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_writing():\n\u001b[1;32m    746\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt read from the ZIP file while there \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    747\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis an open writing handle on it. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    748\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClose the writing handle before trying to read.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 749\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseek\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pos\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    750\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mread(n)\n\u001b[1;32m    751\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "\u001b[0;31mTypeError\u001b[0m: '_thread.RLock' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# fully connected MLP neural network\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
    "    roc_curve, auc\n",
    ")\n",
    "\n",
    "# === CONFIG ===\n",
    "data_dir = '/storage/projects1/e19-4yp-mi-eeg-for-bci/ashan/processed_data2'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# === Load Files and Labels ===\n",
    "npz_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.npz')])\n",
    "\n",
    "all_labels = []\n",
    "for file in npz_files:\n",
    "    y = np.load(file)['y']\n",
    "    all_labels.extend(y)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "classes = np.unique(all_labels)\n",
    "class_weights_array = compute_class_weight(class_weight='balanced', classes=classes, y=all_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float32).to(device)\n",
    "\n",
    "train_files, test_files = train_test_split(npz_files, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# === Fit Scaler ===\n",
    "scaler = StandardScaler()\n",
    "sample_data = []\n",
    "for file in train_files[:3]:\n",
    "    X = np.load(file)['X']\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    sample_data.append(X)\n",
    "scaler.fit(np.vstack(sample_data))\n",
    "\n",
    "# === Model ===\n",
    "class ComplexEEGNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ComplexEEGNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Initialize Model ===\n",
    "input_dim = sample_data[0].shape[1]\n",
    "model = ComplexEEGNet(input_dim=input_dim, num_classes=len(classes)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# === Train Model ===\n",
    "train_losses = []\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for file in train_files:\n",
    "        data = np.load(file)\n",
    "        X = data['X'].reshape(data['X'].shape[0], -1)\n",
    "        y = data['y']\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "        for start in range(0, len(X_tensor), batch_size):\n",
    "            end = start + batch_size\n",
    "            X_batch = X_tensor[start:end]\n",
    "            y_batch = y_tensor[start:end]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    avg_loss = epoch_loss / len(train_files)\n",
    "    train_losses.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def evaluate(files, label=\"Test\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred, y_scores = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for file in files:\n",
    "            data = np.load(file)\n",
    "            X = data['X'].reshape(data['X'].shape[0], -1)\n",
    "            y = data['y']\n",
    "            X = scaler.transform(X)\n",
    "\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "            outputs = model(X_tensor)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            predictions = torch.argmax(probs, dim=1).cpu().numpy()\n",
    "\n",
    "            y_true.extend(y)\n",
    "            y_pred.extend(predictions)\n",
    "            y_scores.extend(probs[:, 1].cpu().numpy())  # For ROC AUC (class 1)\n",
    "\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"\\nðŸ“Š {label} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Pronation', 'Supination']))\n",
    "    return np.array(y_true), np.array(y_pred), np.array(y_scores)\n",
    "\n",
    "# === Final Evaluation ===\n",
    "y_true_train, y_pred_train, _ = evaluate(train_files, \"Train\")\n",
    "y_true_test, y_pred_test, y_scores_test = evaluate(test_files, \"Test\")\n",
    "\n",
    "# === Confusion Matrix ===\n",
    "cm = confusion_matrix(y_true_test, y_pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Pronation', 'Supination'])\n",
    "\n",
    "# === ROC Curve ===\n",
    "fpr, tpr, _ = roc_curve(y_true_test, y_scores_test)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# === Plotting ===\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Loss Curve\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(range(1, epochs+1), train_losses, marker='o')\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "\n",
    "# ROC Curve\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "\n",
    "# Confusion Matrix\n",
    "plt.subplot(1, 3, 3)\n",
    "disp.plot(ax=plt.gca(), cmap='Blues', colorbar=False)\n",
    "plt.title(\"Confusion Matrix Heatmap\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be56b7ed-ad51-4790-aed3-aad57006125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Epoch 2/20\n",
      "Epoch 3/20\n",
      "Epoch 4/20\n",
      "Epoch 5/20\n",
      "Epoch 6/20\n",
      "Epoch 7/20\n",
      "Epoch 8/20\n",
      "Epoch 9/20\n",
      "Epoch 10/20\n",
      "Epoch 11/20\n",
      "Epoch 12/20\n",
      "Epoch 13/20\n",
      "Epoch 14/20\n",
      "Epoch 15/20\n",
      "Epoch 16/20\n",
      "Epoch 17/20\n",
      "Epoch 18/20\n",
      "Epoch 19/20\n",
      "Epoch 20/20\n",
      "\n",
      "ðŸ“Š Train Accuracy: 0.9940\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   pronation       0.99      1.00      0.99      2578\n",
      "  supination       1.00      0.99      0.99      2576\n",
      "\n",
      "    accuracy                           0.99      5154\n",
      "   macro avg       0.99      0.99      0.99      5154\n",
      "weighted avg       0.99      0.99      0.99      5154\n",
      "\n",
      "\n",
      "ðŸ“Š Test Accuracy: 0.5065\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   pronation       0.51      0.51      0.51       693\n",
      "  supination       0.51      0.50      0.50       691\n",
      "\n",
      "    accuracy                           0.51      1384\n",
      "   macro avg       0.51      0.51      0.51      1384\n",
      "weighted avg       0.51      0.51      0.51      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MLP complex neural network on processed data 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# === CONFIG ===\n",
    "data_dir = '/storage/projects1/e19-4yp-mi-eeg-for-bci/ashan/processed_data3'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# === Load File List and Labels ===\n",
    "npz_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.npz')])\n",
    "\n",
    "all_labels = []\n",
    "for file in npz_files:\n",
    "    y = np.load(file)['y']\n",
    "    all_labels.extend(y)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "classes = np.unique(all_labels)\n",
    "class_weights_array = compute_class_weight(class_weight='balanced', classes=classes, y=all_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float32).to(device)\n",
    "\n",
    "# === Train-Test Split ===\n",
    "train_files, test_files = train_test_split(npz_files, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# === Fit Scaler on Training Data ===\n",
    "scaler = StandardScaler()\n",
    "sample_data = []\n",
    "for file in train_files[:3]:\n",
    "    X = np.load(file)['X']\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    sample_data.append(X)\n",
    "scaler.fit(np.vstack(sample_data))\n",
    "\n",
    "# === Define a More Complex Model ===\n",
    "class ComplexEEGNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ComplexEEGNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Train the Model ===\n",
    "model = ComplexEEGNet(input_dim=sample_data[0].shape[1], num_classes=len(classes)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# === Training Loop ===\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for file in train_files:\n",
    "        data = np.load(file)\n",
    "        X = data['X'].reshape(data['X'].shape[0], -1)\n",
    "        y = data['y']\n",
    "        X = scaler.transform(X)\n",
    "\n",
    "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "        y_tensor = torch.tensor(y, dtype=torch.long).to(device)\n",
    "\n",
    "        for start in range(0, len(X_tensor), batch_size):\n",
    "            end = start + batch_size\n",
    "            X_batch = X_tensor[start:end]\n",
    "            y_batch = y_tensor[start:end]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "# === Evaluation Function ===\n",
    "def evaluate(files, label=\"Test\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for file in files:\n",
    "            data = np.load(file)\n",
    "            X = data['X'].reshape(data['X'].shape[0], -1)\n",
    "            y = data['y']\n",
    "            X = scaler.transform(X)\n",
    "\n",
    "            X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
    "            outputs = model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "\n",
    "            y_true.extend(y)\n",
    "            y_pred.extend(predictions)\n",
    "\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"\\nðŸ“Š {label} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['pronation', 'supination']))\n",
    "    return acc\n",
    "\n",
    "# === Final Evaluation ===\n",
    "train_acc = evaluate(train_files, \"Train\")\n",
    "test_acc = evaluate(test_files, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0daf1ae5-9477-48e1-9ea2-cf6e247884e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/projects1/e19-4yp-mi-eeg-for-bci/env/eeg-env/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 â€” Train loss: 0.7296, Train acc: 0.5046 â€” Val loss: 0.6991, Val acc: 0.4899\n",
      "Epoch 2/50 â€” Train loss: 0.6983, Train acc: 0.5370 â€” Val loss: 0.7027, Val acc: 0.4855\n",
      "Epoch 3/50 â€” Train loss: 0.6688, Train acc: 0.5810 â€” Val loss: 0.7140, Val acc: 0.4925\n",
      "Epoch 4/50 â€” Train loss: 0.6451, Train acc: 0.6341 â€” Val loss: 0.7327, Val acc: 0.4811\n",
      "Epoch 5/50 â€” Train loss: 0.6033, Train acc: 0.6848 â€” Val loss: 0.7697, Val acc: 0.4820\n",
      "Epoch 6/50 â€” Train loss: 0.5321, Train acc: 0.7334 â€” Val loss: 0.8176, Val acc: 0.4820\n",
      "Epoch 7/50 â€” Train loss: 0.4765, Train acc: 0.7769 â€” Val loss: 0.9040, Val acc: 0.4758\n",
      "Epoch 8/50 â€” Train loss: 0.4357, Train acc: 0.8088 â€” Val loss: 0.8994, Val acc: 0.4785\n",
      "Early stopping triggered after 8 epochs.\n",
      "\n",
      "ðŸ“Š Train Accuracy: 0.6236\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   pronation       0.61      0.69      0.65      2005\n",
      "  supination       0.64      0.56      0.60      2012\n",
      "\n",
      "    accuracy                           0.62      4017\n",
      "   macro avg       0.63      0.62      0.62      4017\n",
      "weighted avg       0.63      0.62      0.62      4017\n",
      "\n",
      "\n",
      "ðŸ“Š Validation Accuracy: 0.4899\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   pronation       0.49      0.56      0.52       573\n",
      "  supination       0.48      0.42      0.45       564\n",
      "\n",
      "    accuracy                           0.49      1137\n",
      "   macro avg       0.49      0.49      0.49      1137\n",
      "weighted avg       0.49      0.49      0.49      1137\n",
      "\n",
      "\n",
      "ðŸ“Š Test Accuracy: 0.5166\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   pronation       0.51      0.63      0.57       693\n",
      "  supination       0.52      0.40      0.45       691\n",
      "\n",
      "    accuracy                           0.52      1384\n",
      "   macro avg       0.52      0.52      0.51      1384\n",
      "weighted avg       0.52      0.52      0.51      1384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
    "\n",
    "# === CONFIG ===\n",
    "data_dir = '/storage/projects1/e19-4yp-mi-eeg-for-bci/ashan/processed_data3'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 50\n",
    "batch_size = 64\n",
    "patience = 7  # Early stopping patience\n",
    "\n",
    "# === Custom Dataset to load npz files chunk-wise ===\n",
    "class EEGDataset(Dataset):\n",
    "    def __init__(self, files, scaler=None):\n",
    "        self.files = files\n",
    "        self.scaler = scaler\n",
    "        self.data = []\n",
    "        self.labels = []\n",
    "        self._load_all()\n",
    "\n",
    "    def _load_all(self):\n",
    "        # Load all data into memory - for smaller datasets\n",
    "        # If dataset is too large, consider loading batch-wise from disk instead\n",
    "        all_X, all_y = [], []\n",
    "        for f in self.files:\n",
    "            data = np.load(f)\n",
    "            X = data['X'].reshape(data['X'].shape[0], -1)\n",
    "            y = data['y']\n",
    "            all_X.append(X)\n",
    "            all_y.append(y)\n",
    "        X = np.vstack(all_X)\n",
    "        y = np.hstack(all_y)\n",
    "        if self.scaler:\n",
    "            X = self.scaler.transform(X)\n",
    "        self.data = torch.tensor(X, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# === Load File List and Labels ===\n",
    "npz_files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir) if f.endswith('.npz')])\n",
    "\n",
    "all_labels = []\n",
    "for file in npz_files:\n",
    "    y = np.load(file)['y']\n",
    "    all_labels.extend(y)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "classes = np.unique(all_labels)\n",
    "class_weights_array = compute_class_weight(class_weight='balanced', classes=classes, y=all_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights_array, dtype=torch.float32).to(device)\n",
    "\n",
    "# === Split data into train, validation, test sets ===\n",
    "train_val_files, test_files = train_test_split(npz_files, test_size=0.2, random_state=42, shuffle=True)\n",
    "train_files, val_files = train_test_split(train_val_files, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "# === Fit Scaler on Training Data ===\n",
    "scaler = StandardScaler()\n",
    "sample_data = []\n",
    "for file in train_files:\n",
    "    X = np.load(file)['X'].reshape(-1, np.load(file)['X'].shape[1]*np.load(file)['X'].shape[2])\n",
    "    sample_data.append(X)\n",
    "scaler.fit(np.vstack(sample_data))\n",
    "\n",
    "# === Create Dataset and DataLoader ===\n",
    "train_dataset = EEGDataset(train_files, scaler=scaler)\n",
    "val_dataset = EEGDataset(val_files, scaler=scaler)\n",
    "test_dataset = EEGDataset(test_files, scaler=scaler)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# === Define Model ===\n",
    "class ComplexEEGNet(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(ComplexEEGNet, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# === Initialize Model, Loss, Optimizer, Scheduler ===\n",
    "input_dim = train_dataset.data.shape[1]\n",
    "model = ComplexEEGNet(input_dim=input_dim, num_classes=len(classes)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)  # Weight decay for L2 regularization\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "# === Training with Early Stopping ===\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * X_batch.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        correct += (preds == y_batch).sum().item()\n",
    "        total += y_batch.size(0)\n",
    "\n",
    "    train_loss /= total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            val_loss += loss.item() * X_batch.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            val_correct += (preds == y_batch).sum().item()\n",
    "            val_total += y_batch.size(0)\n",
    "\n",
    "    val_loss /= val_total\n",
    "    val_acc = val_correct / val_total\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{epochs} â€” Train loss: {train_loss:.4f}, Train acc: {train_acc:.4f} â€” Val loss: {val_loss:.4f}, Val acc: {val_acc:.4f}\")\n",
    "\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping triggered after {epoch+1} epochs.\")\n",
    "            break\n",
    "\n",
    "# === Load best model for testing ===\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# === Evaluation function ===\n",
    "def evaluate(loader, label=\"Test\"):\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            y_true.extend(y_batch.cpu().numpy())\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "    acc = np.mean(np.array(y_true) == np.array(y_pred))\n",
    "    print(f\"\\nðŸ“Š {label} Accuracy: {acc:.4f}\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['pronation', 'supination']))\n",
    "    return acc\n",
    "\n",
    "# === Final evaluation ===\n",
    "train_acc = evaluate(train_loader, \"Train\")\n",
    "val_acc = evaluate(val_loader, \"Validation\")\n",
    "test_acc = evaluate(test_loader, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08e46f-3961-40e3-bf6a-99174cad7789",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg-env",
   "language": "python",
   "name": "eeg-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
